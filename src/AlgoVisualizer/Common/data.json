{
  "sortingInstructions": "Select the sorting algorithm you want to visualize from the dropdown menu. \n Use the slider to adjust the array size / the visualization speed.\n Click the \"Visualize\" button to start the sorting algorithm. \n To start a new sorting visualization, click the \"Restart\" button and repeat the process.",
  "pathFindingInstructions": "Drag and drop the start and finish icons to different cells. \n Place obstacles on the grid by clicking on the cells. \n Choose one of the path-finding algorithms from the dropdown menu.\n Use the slider to adjust the speed of the algorithm visualization.\n Click the \"Visualize\" button to start the algorithm.\n To start a new visualization, click the \"Restart\" button and repeat the process.",
  "BS": {
    "desc": "Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, indicating that the list is sorted. This algorithm is named \"Bubble Sort\" because smaller elements \"bubble\" to the top of the list while larger elements \"sink\" to the bottom.",
    "howItWorks": "1. Start at the beginning of the list. \n  2. Compare the first two elements. If the first element is larger than the second, swap them.\n 3. Move to the next pair of elements (from left to right) and repeat step 2.\n 4. Continue this process for each pair of adjacent elements until you reach the end of the list.\n 5. After one pass through the list, the largest unsorted element will have \"bubbled up\" to the end of the list.\n 6. Repeat steps 1-5 for the remaining unsorted portion of the list, excluding the already sorted elements.\n 7. Continue these steps until no more swaps are needed, indicating that the list is sorted.\n",
    "timeComp": "• Best-case time complexity: O(n) (when the list is already sorted)\n • Average-case time complexity: O(n^2)\n • Worst-case time complexity: O(n^2) (when the list is sorted in reverse order)\n"
  },
  "QS": {
    "desc": "Quick Sort is a highly efficient and widely used sorting algorithm that uses a <a href=\"https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm\" target=\"_blank\">divide-and-conquer</a> approach to sort a list or an array of elements. It works by selecting a \"pivot\" element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. Quick Sort is known for its speed and is often the preferred choice for sorting large datasets.",
    "howItWorks": "1. Choose a pivot element from the array. The choice of the pivot can affect the algorithm's performance. In our case we choose the last element.\n 2. Partition the array into two sub-arrays: elements less than the pivot and elements greater than the pivot.\n 3. Recursively apply Quick Sort to both sub-arrays, repeating steps 1 and 2.\n 4. Combine the sorted sub-arrays and the pivot to produce the final sorted array.\n",
    "timeComp": "• Best-case time complexity: O(n log n)\n • Average-case time complexity: O(n log n)\n • Worst-case time complexity: O(n^2) when the pivot selection consistently results in unbalanced partitions."
  },
  "MS": {
    "desc": "Merge Sort is a highly efficient and stable sorting algorithm that divides a list or an array into smaller sub-arrays, sorts those sub-arrays, and then combines them to produce a sorted result. It follows the <a href=\"https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm\" target=\"_blank\">divide-and-conquer</a> approach, making it a reliable choice for sorting large datasets.",
    "howItWorks": "1. Divide: Divide the unsorted list into two equal sub-lists until each sub-list contains only one element or is empty.\n 2. Conquer: Recursively sort each sub-list. This is done by applying the Merge Sort algorithm to each of the sub-lists, further dividing them until they consist of one or zero elements.\n 3. Combine (Merge): Merge the sorted sub-lists back together in a way that maintains the order. Compare elements from both sub-lists and place them in the correct order in a new merged list.\n 4. Repeat the divide, conquer, and combine steps until you have a single sorted list.\n",
    "timeComp": "• Best-case time complexity: O(n log n) \n• Average-case time complexity: O(n log n)\n • Worst-case time complexity: O(n log n)\n"
  },
  "HS": {
    "desc": "Heap Sort is a comparison-based sorting algorithm that uses a <a href=\"https://en.wikipedia.org/wiki/Binary_heap\" target=\"_blank\">binary heap</a> data structure to sort elements efficiently. It is known for its reliable time complexity and is often used when a stable sort is not required. Heap Sort works by transforming the input data into a binary heap, which is then repeatedly converted into a sorted array.",
    "howItWorks": "1. Build a Max-Heap: The first step is to build a binary max-heap from the input data. This is done by repeatedly \"heapifying\" the array. Heapifying involves ensuring that the parent node of every subtree has a greater value than its children.\n 2. Swap and Remove: After the max-heap is constructed, the largest element (the root) is at the top of the heap. Swap it with the last element in the heap and decrease the heap size. The swapped element is now in its correct sorted position.\n 3. Re-Heapify: Heapify the remaining elements in the heap, which have been reduced in size by one after the previous step. This ensures that the next largest element moves to the top.\n 4. Repeat: Repeat steps 2 and 3 until the entire array is sorted. The largest elements keep moving to the end of the array, and the sorted portion grows from the end.\n",
    "timeComp": "• Best-case time complexity: O(n log n) \n • Average-case time complexity: O(n log n)\n • Worst-case time complexity: O(n log n)\n"
  },
  "SS": {
    "desc": "Selection Sort is a straightforward and elementary sorting algorithm that works by dividing the input list into two parts: the sorted and the unsorted portions. It repeatedly selects the smallest (or largest) element from the unsorted part and moves it to its correct position in the sorted part. The algorithm continues this process until the entire list is sorted.",
    "howItWorks": "1. Initial State: The entire list is considered unsorted initially.\n 2. Find the Minimum: Search the unsorted part of the list to find the smallest element.\n 3. Swap: Swap the found minimum element with the first element in the unsorted part. This effectively moves the minimum element to the end of the sorted part.\n 4. Expand the Sorted Part: Increase the boundary between the sorted and unsorted parts by one element. The sorted part grows, and the unsorted part shrinks.\n 5. Repeat: Repeat steps 2-4 until the entire list is sorted.\n",
    "timeComp": "• Best-case time complexity: O(n^2) \n • Average-case time complexity: O(n^2)\n • Worst-case time complexity: O(n^2)\n"
  },
  "DIJKSTRA": {
    "desc": "Dijkstra's Algorithm is a widely used and well-known pathfinding algorithm that finds the shortest path between two nodes in a weighted graph. It is named after its inventor, Dutch computer scientist Edsger W. Dijkstra. This algorithm is particularly useful for finding the shortest path in networks with non-negative edge weights.",
    "howItWorks": "1. Initialization: The algorithm starts by assigning a tentative distance of 0 to the starting node and an infinite distance to all other nodes. It maintains a list of unvisited nodes\n 2. Exploration: It repeatedly selects the unvisited nodes with the smallest tentative distance. Think of it as choosing the next stop that's closest to your destination.\n 3. Update Distances: For the selected node, it calculates the distances to its unvisited neighbors through the current node. If this new distance is shorter than the previously assigned distance, it updates it.\n 4. Mark as Visited: The algorithm marks the current node as visited, ensuring it won't be revisited.\n 5. Repeat: Steps 2 to 4 are repeated until the algorithm reaches the destination node, or it has explored all possible paths.\n 6. Backtracking: Once the destination is reached, the algorithm can backtrack from the destination to the starting node, following the path with the smallest total distance. This path is the shortest route.\n",
    "timeComp": "• Dijkstra's Algorithm with adjacency Matrix: O(V^2), where V is the number of nodes. \n• Dijkstra's Algorithm with a priority queue / min-heap: O(|E| + |V| log |V|), where |E| is the number of edges.\n "
  },
  "AS": {
    "desc": "The A* (pronounced \"A star\") algorithm is a widely-used pathfinding algorithm in computer science and artificial intelligence. It is designed to find the shortest path between two nodes in a graph, while considering both the cost to reach a particular node and a <a href=\"https://en.wikipedia.org/wiki/Heuristic_(computer_science)\" target=\"_blank\">heuristic</a> estimate of the remaining cost to reach the goal. A* is especially efficient when searching for paths in applications like maps, video games, and robotics.",
    "howItWorks": "1. Initialization: The algorithm starts by assigning a tentative distance of 0 to the starting node and calculates an estimated cost to reach the goal. It maintains a list of unvisited nodes and calculates an \"f-score\" for each node, which is the sum of the tentative distance and the estimated cost.\n 2. Exploration: It repeatedly selects the unvisited nodes with the lowest f-score. This is where A* combines elements from Dijkstra's Algorithm, as it considers the actual cost to reach the node, and GBFS, as it uses the estimated cost to the goal. It's like choosing the next stop that's both close to your current location and in the right direction toward your destination.\n 3. Update Distances: For the selected node, it calculates the distances and f-scores for its unvisited neighbors. If this new distance is shorter than the previously assigned distance, it updates it. The algorithm also tracks the path to each node.\n 4. Mark as Visited: The algorithm marks the current node as visited, ensuring it won't be revisited.\n 5. Repeat: Steps 2 to 4 are repeated until the algorithm reaches the destination node, or it has explored all possible paths.\n 6. Backtracking: Once the destination is reached, the algorithm can backtrack from the destination to the starting node, following the path with the smallest total distance. This path is the shortest route.\n",
    "timeComp": "• A* Algorithm with adjacency Matrix: O(V^2), where V is the number of nodes. \n• A* Algorithm with a priority queue / min-heap: O(|E| + |V| log |V|), where |E| is the number of edges.\n "
  },
  "BFS": {
    "desc": "Breadth-First Search is a fundamental graph traversal and pathfinding algorithm used to explore and find the shortest path in unweighted graphs. It operates by systematically exploring all the nodes in layers, starting from the initial node and moving outward. BFS is an excellent choice for finding the shortest path in scenarios like routing on a map with uniform edge costs, or searching for solutions in a state space.",
    "howItWorks": "1. Initialization: Begin by selecting a starting node and mark it as visited. This node becomes the first layer of nodes.\n 2. Explore Neighbors: Explore all the neighbors of the current layer of nodes. These neighbors become the next layer.\n 3. Mark as Visited: Mark the neighbors as visited and continue exploring outward.\n 4. Layer by Layer: Continue this process layer by layer until you reach the destination node or explore the entire graph. BFS ensures that you explore all nodes at the current layer before moving on to the next.\n 5. Shortest Path: If the destination node is found, you can backtrack from the destination to the start, following the marked path to reconstruct the shortest path.\n",
    "timeComp": "• Best-case time complexity: O(1) - When the destination node is very close to the starting node.\n • Average-case time complexity: O(V + E) - In most cases, where V is the number of nodes (vertices) and E is the number of connections (edges) in the graph.\n • Worst-case time complexity: O(V + E) - In the worst case, BFS explores all nodes and edges.\n"
  },
  "DFS": {
    "desc": "Depth-First Search is a fundamental graph traversal and pathfinding algorithm used to explore and traverse through graphs or trees. It is often used to explore as far down a branch as possible before backtracking. DFS can be implemented using either recursion or a stack data structure.",
    "howItWorks": "1. Initialization: Begin by selecting a starting node and mark it as visited. This node becomes the current node.\n 2. Explore Neighbors: Explore one of the unvisited neighbors of the current node. It typically selects the first unvisited neighbor it encounters and continues deeper into the graph.\n 3. Recurse or Stack: If using recursion, recursively apply the DFS algorithm to the selected neighbor. If using a stack, push the neighbor onto the stack and make it the current node.\n 4. Backtrack: If the current node has no unvisited neighbors or the recursion reaches a leaf node, backtrack to the previous node or pop the node from the stack.\n 5. Continue Exploration: Continue the process until all nodes have been visited or until the destination node is reached.\n 6. Path Reconstruction:  If the destination node is found, you can backtrack from the destination to the start, following the marked path to reconstruct the shortest path.\n",
    "timeComp": "• Best-case time complexity: O(1) - When the destination node is the first neighbor.\n • Average-case time complexity: O(V + E) - In most cases, where V is the number of nodes (vertices) and E is the number of connections (edges) in the graph.\n • Worst-case time complexity: O(V + E) - In the worst case, DFS explores all nodes and edges.\n"
  },
  "GBFS": {
    "desc": "Greedy Best-First Search is a heuristic-based algorithm used for graph traversal and pathfinding. It prioritizes nodes based on a heuristic function that estimates how close a node is to the goal. GBFS is particularly suitable for scenarios where finding a quick solution is more important than finding the optimal one.",
    "howItWorks": "1. Initialization: Begin by selecting a starting node and mark it as visited. This node becomes the current node.\n 2. Heuristic Evaluation: Calculate a heuristic value for each unvisited neighbor of the current node. The heuristic value is an estimate of how close each neighbor is to the goal.\n 3. Node Selection: Choose the unvisited neighbor with the lowest heuristic value as the next node to explore. GBFS is \"greedy\" because it always selects the neighbor that appears to be the closest to the goal, regardless of the actual path cost.\n 4. Repeat: Continue the process by making the selected neighbor the new current node and repeating steps 2 and 3 until the destination node is reached or all nodes have been explored.\n 5. Path Reconstruction: If the destination node is reached, backtrack from the destination to the start to reconstruct the path.\n",
    "timeComp": "• Best-case time complexity: O(1) - When the destination node is the first neighbor with the lowest heuristic value.\n • Average-case time complexity: O(b^d) - Where \"b\" is the branching factor (average number of successors per node) and \"d\" is the depth of the finish node in the search tree.\n • Worst-case time complexity: O(b^D) - Where \"D\" is the maximum depth of the search tree. GBFS can perform poorly in cases where the heuristic is not informative.\n"
  }
}
